{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYgYqDkl7C3FFIko4695bC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ishara20/Text_analysis_using_Python/blob/main/Text_Analysis_For_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yNZhrrX76bf",
        "outputId": "c61fa7ae-1472-4265-ae98-339e8bc2b37b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJgAVF3a8DMy",
        "outputId": "736a301a-06c3-4c61-8af8-90e894fe19d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read course descriptions\n",
        "with open(\"Course-Descriptions.txt\", 'r') as fh:\n",
        "    descriptions = fh.read().splitlines()\n",
        "print(\"Sample course descriptions :\", descriptions[:2])\n",
        "\n",
        "#Setup stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#setup wordnet for lemmatization\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#Custom tokenizer that will perform tokenization, stopword removal\n",
        "#and lemmatization\n",
        "def customtokenize(str):\n",
        "    tokens=nltk.word_tokenize(str)\n",
        "    nostop = list(filter(lambda token: token not in stopwords.words('english'), tokens))\n",
        "    lemmatized=[lemmatizer.lemmatize(word) for word in nostop ]\n",
        "    return lemmatized\n",
        "\n",
        "#Generate TFIDF matrix\n",
        "vectorizer = TfidfVectorizer(tokenizer=customtokenize)\n",
        "tfidf=vectorizer.fit_transform(descriptions)\n",
        "\n",
        "print(\"\\nSample feature names identified : \", vectorizer.get_feature_names_out()[:25])\n",
        "print(\"\\nSize of TFIDF matrix : \",tfidf.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_47zAwV8H1E",
        "outputId": "9116cda4-f2f8-4229-c24a-c9e67e5fdd09"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample course descriptions : ['In this practical, hands-on course, learn how to do data preparation, data munging, data visualization, and predictive analytics. ', 'PHP is the most popular server-side language used to build dynamic websites, and though it is not especially difficult to use, nonprogrammers often find it intimidating. ']\n",
            "\n",
            "Sample feature names identified :  [\"'ll\" \"'re\" \"'s\" '(' ')' ',' '.' '?' 'actively' 'adopting' 'amazon'\n",
            " 'analysis' 'analytics' 'application' 'applied' 'architect' 'architecture'\n",
            " 'around' 'aspect' 'associate' 'aws' 'basic' 'become' 'begin' 'big']\n",
            "\n",
            "Size of TFIDF matrix :  (20, 238)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building the model"
      ],
      "metadata": {
        "id": "88HUrtj89TTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the pre-built classifications for training\n",
        "with open(\"Course-Classification.txt\", 'r') as fh:\n",
        "    classifications = fh.read().splitlines()\n",
        "\n",
        "#Create Labels and integer classes\n",
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(classifications)\n",
        "print(\"Classes found : \", le.classes_)\n",
        "\n",
        "#Convert classes to integers for use with ML\n",
        "int_classes = le.transform(classifications)\n",
        "print(\"\\nClasses converted to integers :\", int_classes)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#Split as training and testing sets\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(tfidf, int_classes,random_state=0)\n",
        "\n",
        "#Build the model\n",
        "classifier= MultinomialNB().fit(xtrain, ytrain)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT1LFZd-9WP7",
        "outputId": "12287eb9-262f-463f-af72-776b9aaf6819"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes found :  ['Cloud-Computing' 'Data-Science' 'Programming']\n",
            "\n",
            "Classes converted to integers : [1 2 2 0 1 2 1 2 0 1 1 2 2 0 2 0 0 0 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running Predictions"
      ],
      "metadata": {
        "id": "Lz_ndEuj9v6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(\"Testing with Test Data :\\n------------------------\")\n",
        "#Predict on test data\n",
        "predictions=classifier.predict(xtest)\n",
        "print(\"Confusion Matrix : \")\n",
        "print(metrics.confusion_matrix(ytest, predictions))\n",
        "print(\"\\n Prediction Accuracy : \",  \\\n",
        "      metrics.accuracy_score(ytest, predictions) )\n",
        "\n",
        "print(\"\\nTesting with Full Corpus :\\n--------------------------\")\n",
        "#Predict on entire corpus data\n",
        "predictions=classifier.predict(tfidf)\n",
        "print(\"Confusion Matrix : \")\n",
        "print(metrics.confusion_matrix(int_classes, predictions))\n",
        "print(\"\\n Prediction Accuracy : \",  \\\n",
        "      metrics.accuracy_score(int_classes, predictions) )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j44xkOSP9zc2",
        "outputId": "2fd189f4-7872-4ff6-a87b-8a73a5d2a25c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with Test Data :\n",
            "------------------------\n",
            "Confusion Matrix : \n",
            "[[1 0 0]\n",
            " [0 0 1]\n",
            " [1 0 2]]\n",
            "\n",
            " Prediction Accuracy :  0.6\n",
            "\n",
            "Testing with Full Corpus :\n",
            "--------------------------\n",
            "Confusion Matrix : \n",
            "[[6 0 0]\n",
            " [0 4 1]\n",
            " [1 0 8]]\n",
            "\n",
            " Prediction Accuracy :  0.9\n"
          ]
        }
      ]
    }
  ]
}