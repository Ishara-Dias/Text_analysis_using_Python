{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7bp+ACT46R5uV0dEuNC/R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ishara20/Text_analysis_using_Python/blob/main/Preparing_Data_For_Predictive_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nJaX52mCsQL",
        "outputId": "64a0cdc5-120d-40d2-b8c8-b7df1ed1c619"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLDfyhWjAyzV",
        "outputId": "7cef6c13-90cd-4ea8-853f-958c66b0e69e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text read from file :  In this practical, hands-on course, learn how to do data preparation, data munging, data visualization, and predictive analytics. \n",
            "PHP is the most popular server-side language used to build dynamic we\n",
            "\n",
            "Sample token list :  ['in', 'this', 'practical', 'hands-on', 'course', 'learn', 'how', 'to', 'do', 'data']\n",
            "\n",
            "Total Tokens :  576\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "#Read course description file\n",
        "base_file = open(\"Course-Descriptions.txt\", 'rt')\n",
        "raw_text = base_file.read()\n",
        "base_file.close()\n",
        "print(\"Text read from file : \",raw_text[:200])\n",
        "\n",
        "#tokenization\n",
        "import nltk\n",
        "token_list = nltk.word_tokenize(raw_text)\n",
        "\n",
        "#Replace special characters\n",
        "token_list2 = [word.replace(\"'\", \"\") for word in token_list ]\n",
        "\n",
        "#Remove punctuations\n",
        "token_list3 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list2))\n",
        "\n",
        "#Convert to lower case\n",
        "token_list4=[word.lower() for word in token_list3 ]\n",
        "\n",
        "print(\"\\nSample token list : \", token_list4[:10])\n",
        "print(\"\\nTotal Tokens : \",len(token_list4))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the ngrams DB"
      ],
      "metadata": {
        "id": "qxWsXClRCMuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "#Use a sqlite database to store ngrams information\n",
        "import sqlite3\n",
        "conn = sqlite3.connect(\":memory:\")\n",
        "\n",
        "#table to store first word, second word and count of occurance\n",
        "conn.execute('''DROP TABLE IF EXISTS NGRAMS''')\n",
        "conn.execute('''CREATE TABLE NGRAMS\n",
        "         (FIRST   TEXT  NOT NULL,\n",
        "          SECOND  TEXT  NOT NULL,\n",
        "          COUNTS  INT   NOT NULL,\n",
        "         CONSTRAINT PK_GRAMS PRIMARY KEY (FIRST,SECOND));''')\n",
        "\n",
        "#Generate bigrams\n",
        "bigrams = ngrams(token_list4,2)\n",
        "\n",
        "#Store bigrams in DB\n",
        "for i in bigrams:\n",
        "    insert_str=\"INSERT INTO NGRAMS (FIRST,SECOND,COUNTS) \\\n",
        "          VALUES ('\" + i[0] + \"','\" + i[1] + \"',1 ) \\\n",
        "          ON CONFLICT(FIRST,SECOND) DO UPDATE SET COUNTS=COUNTS + 1\"\n",
        "    conn.execute(insert_str);\n",
        "\n",
        "#Look at sample data from the table\n",
        "cursor = conn.execute(\"SELECT FIRST, SECOND, COUNTS from NGRAMS LIMIT 5\")\n",
        "for gram_row in cursor:\n",
        "    print(\"FIRST=\", gram_row[0], \"SECOND=\",gram_row[1],\"COUNT=\",gram_row[2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iA4sa7pCOUF",
        "outputId": "f915cf99-5d65-4a90-9b9a-42d7c230c88f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIRST= in SECOND= this COUNT= 2\n",
            "FIRST= this SECOND= practical COUNT= 1\n",
            "FIRST= practical SECOND= hands-on COUNT= 1\n",
            "FIRST= hands-on SECOND= course COUNT= 1\n",
            "FIRST= course SECOND= learn COUNT= 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Recommending next word"
      ],
      "metadata": {
        "id": "cfxO06L7CZhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to query DB and find next word\n",
        "def recommend(str):\n",
        "    nextwords = []\n",
        "    #Find next words, sort them by most occurance\n",
        "    cur_filter = conn.execute(\"SELECT SECOND from NGRAMS \\\n",
        "                              WHERE FIRST='\" + str + \"' \\\n",
        "                              ORDER BY COUNTS DESC\")\n",
        "\n",
        "    #Build a list ordered from most frequent to least frequent next word\n",
        "    for filt_row in cur_filter:\n",
        "        nextwords.append(filt_row[0])\n",
        "    return nextwords\n",
        "\n",
        "#Recommend for words data and science\n",
        "print(\"Next word for data \", recommend(\"data\"))\n",
        "print(\"\\nNext word for science \", recommend(\"science\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOCAzVdjCb4r",
        "outputId": "8912a842-235e-4850-dc55-dd8779b7bb85"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next word for data  ['science', 'analysis', 'data', 'from', 'in', 'mining', 'munging', 'node.js', 'preparation', 'scientists', 'visualization', 'you']\n",
            "\n",
            "Next word for science  ['begins', 'requires', 'specialists', 'teams']\n"
          ]
        }
      ]
    }
  ]
}